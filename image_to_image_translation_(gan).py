# -*- coding: utf-8 -*-
"""Image-to-image translation (GAN).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AlEJakscdfLfy2iFQFoqSKXJO_9j-Cb0
"""

import tensorflow as tf
tf.test.gpu_device_name()

"""# **Import libraries**

for Read dicom images
"""

pip install pydicom

pip install SimpleITK

# Commented out IPython magic to ensure Python compatibility.
# visualisation
import matplotlib.pyplot as plt
import seaborn as sns
import cv2
import pydicom
import pydicom as dicom
from skimage import io
import os

# handle table-like data and matrices
import pandas as pd
import numpy as np
import glob
import random
import tensorflow.keras.backend as kb

# ignore warnings
import warnings
warnings.filterwarnings('ignore')

# model
from keras import backend as K
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import tensorflow as tf
from tensorflow.keras.layers import *
from keras.callbacks import ModelCheckpoint
from tensorflow.keras.utils import plot_model
from tensorflow.keras import Model
from tensorflow.keras.models import load_model, save_model

# Bring in the sequential api for the generator and discriminator
from tensorflow.keras.models import Sequential

# Bring in the layers for the neural network
from tensorflow.keras.layers import Conv2D, Dense, Flatten, Reshape, LeakyReLU, Dropout, UpSampling2D

from tqdm import tqdm
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'

from keras.models import Sequential, Model
from keras.layers import Dense, Flatten, Conv2D, Reshape, Input, Conv2DTranspose
from keras.layers import Activation, LeakyReLU, BatchNormalization, Dropout, Resizing
from keras.losses import BinaryCrossentropy
from tensorflow.keras.applications import VGG16
try:
    from tensorflow.keras.optimizers import Adam
except:
    from keras.optimizers import Adam

# import necessary packages
import time
import glob
import matplotlib
import matplotlib.image as mpimg
matplotlib.use('Agg')
# %matplotlib inline
import PIL
import imageio

import SimpleITK as sitk
from IPython import display
from IPython.display import Image
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function
from PIL import Image
from tensorflow.keras.optimizers import Adam
from matplotlib import *
from matplotlib import pyplot
from matplotlib.pyplot import *
import sys
from skimage import metrics
from skimage.metrics import structural_similarity as ssim

"""Content My Drive"""

from google.colab import drive

drive.mount('/content/drive')

"""# **Convert Data From Dicom To Png**"""

dicom_folder = input('input path')
print(dicom_folder)
png_folder = input('output path')
print(png_folder)

for filename in os.listdir(dicom_folder):
    
    if filename.endswith('.dcm'):
        ds = pydicom.dcmread(os.path.join(dicom_folder, filename))
        img = Image.fromarray(ds.pixel_array)
        png_filename = os.path.splitext(filename)[0] + '.png'
        img.save(os.path.join(png_folder, png_filename))

"""# **dealing with data**

Read Data From Drive
"""

Patients_Directory_Path = "/content/drive/MyDrive/Total_Clean_Data_png(3)"
Patients_Directory = os.listdir(Patients_Directory_Path)
Patients_Directory.sort()
print(Patients_Directory)

"""Splitting data"""

train_ratio=1

random.seed(1)

train_patients = random.sample(Patients_Directory, int(len(Patients_Directory)*train_ratio))

""" **Training Dataset**"""

train_images = []

for patient in train_patients:
    patient_path = os.path.join(Patients_Directory_Path, patient)
    print(patient_path)
    for filename in os.listdir(patient_path):
        f_path = os.path.join(patient_path, filename)
        train_images.append(f_path)

"""check Training Data"""

print(len(train_images))

"""convert to a Numpy array"""

train_size = len(train_images)
training_images=np.array([np.array(Image.open(i),dtype="float32") for i in train_images[0:train_size]])
print('training images',training_images.shape)

"""# **Preprocessing**

Normalize pixel values
"""

training_images = (training_images - 127.5 ) / 127.5

print(training_images[0][8])

"""Make into 4D array"""

training_images=training_images[:,:,:,np.newaxis]

"""Check shape"""

print(training_images.shape)

"""# **Check the brain images**"""

#Check the brains, plot the first 6
pyplot.figure(figsize=(25,25))
for i in range(6):
    pyplot.subplot(5, 3, 1 + i)
    pyplot.axis('off')
    pyplot.title('Brain BTE with size: {}'.format(training_images[0].shape),fontsize=15)
    pyplot.imshow(training_images[i,:,:,0],cmap='gray')
pyplot.show()

"""# **Build Generator and discriminator**"""

def Generator():

    model = tf.keras.Sequential()

    model.add(Dense(32*32*256, input_shape=(256,)))
    model.add(LeakyReLU(alpha=0.2))
    model.add(Reshape((32, 32,256)))

    model.add(Conv2DTranspose(256, (4,4), strides=(2,2), padding='same', use_bias=False))
    model.add(BatchNormalization())
    model.add(LeakyReLU(alpha=0.2))
    
    
    model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same', use_bias=False))
    model.add(BatchNormalization())
    model.add(LeakyReLU(alpha=0.2))
    
    
    model.add(Conv2DTranspose(64, (4,4), strides=(2,2), padding='same', use_bias=False))
    model.add(BatchNormalization())
    model.add(LeakyReLU(alpha=0.2))
    

    model.add(Conv2D(64, (3,3), strides=(1,1), padding='same', use_bias=False))
    model.add(BatchNormalization())
    model.add(LeakyReLU(alpha=0.2))


    model.add(Conv2D(1, (3,3),strides=(1,1), padding='same', use_bias=False))
    
    return model

"""Check Generator Summary"""

g_model = Generator()
g_model.summary()

def Discriminator(input=(256,256,1)):

    model = tf.keras.Sequential()

    model.add(Conv2D(16, (4,4), strides=(2, 2), padding='same',input_shape=input))
    model.add(BatchNormalization())
    model.add(LeakyReLU(alpha=0.2))
    model.add(Dropout(0.4))
    
    model.add(Conv2D(32, (4,4), strides=(2, 2), padding='same'))
    model.add(BatchNormalization())
    model.add(LeakyReLU(alpha=0.2))
    model.add(Dropout(0.4))
    
    
    model.add(Conv2D(64, (4,4), strides=(2, 2), padding='same'))
    model.add(BatchNormalization())
    model.add(LeakyReLU(alpha=0.2))
    model.add(Dropout(0.4))

    model.add(Conv2D(128, (4,4), strides=(2, 2), padding='same'))
    model.add(BatchNormalization())
    model.add(LeakyReLU(alpha=0.2))
    model.add(Dropout(0.4))
    
    model.add(Flatten())
    model.add(Dense(1))
    return model

"""Check Generator Summary"""

d_model = Discriminator()
d_model.summary()

"""# **Visualising generated images(Fake image)**

Number of samples to visualise and number of points in latent space
"""

n_samples=5
latent_dim=256

"""generate noise according to number of samples and latent space"""

noise = tf.random.normal([n_samples, latent_dim])

"""generate fake (noise) images"""

x_fake = g_model(noise,training=False)
pyplot.figure(figsize=(25,25))
for i in range(n_samples):
    pyplot.subplot(5, 5, 1 + i)
    pyplot.axis('off')
    pyplot.title('Generator Noise',fontsize=15)
    pyplot.imshow(x_fake[i, :, :,0],cmap='gray')
pyplot.show()
pyplot.close()

"""# **Test Discriminator**"""

print (x_fake.shape)

d_model.predict(x_fake)

"""# **Define loss functions**"""

cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)

def discriminator_loss(real_output, fake_output):
    real_loss = cross_entropy(tf.ones_like(real_output), real_output)
    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)
    total_loss = real_loss + fake_loss
    return total_loss

def generator_loss(fake_output):
    return cross_entropy(tf.ones_like(fake_output), fake_output)

"""# **Define optimisers**"""

generator_optimiser = tf.keras.optimizers.Adam(learning_rate=0.0002)

discriminator_optimiser = tf.keras.optimizers.Adam(learning_rate=0.0001)

"""# **Training Functions**"""

batch_size = 10

def train_step(images):
    noise = tf.random.normal([batch_size, latent_dim])

    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:
        generated_images = g_model(noise, training=True)

        real_output = d_model(images, training=True)
        fake_output = d_model(generated_images, training=True)

        g_loss = generator_loss(fake_output)
        d_loss = discriminator_loss(real_output, fake_output)

    gradients_of_generator = gen_tape.gradient(g_loss, g_model.trainable_variables)
    gradients_of_discriminator = disc_tape.gradient(d_loss, d_model.trainable_variables)

    generator_optimiser.apply_gradients(zip(gradients_of_generator, g_model.trainable_variables))
    discriminator_optimiser.apply_gradients(zip(gradients_of_discriminator, d_model.trainable_variables))
    return g_loss,d_loss

"""# **Store generator and discriminator Loss Values**"""

d_loss_history = []
g_loss_history = []

"""# **Training Loop**

Batch and shuffle the data
"""

total_size=training_images.shape[0]
train_dataset = tf.data.Dataset.from_tensor_slices(training_images).shuffle(total_size).batch(batch_size)

EPOCHS = 200

batch_per_epoch=np.round(training_images.shape[0]/batch_size)

def train(dataset, epochs):
    
    for epoch in range(epochs):
        count=0
        for image_batch in dataset:
          
            g_loss,d_loss=train_step(image_batch) 

            if (count) % 25 == 0:
     
               print('Batch Per Epoch : %d/%d' % (count ,batch_per_epoch))           

            if (count) % 350 == 0:

                print('>%d, %d/%d, g=%.8f, d=%.8f' % (epoch, count, batch_per_epoch, g_loss, d_loss))
                g_loss_history.append(g_loss)
                d_loss_history.append(d_loss) 
                
                noise = tf.random.normal([n_samples, latent_dim])
                x_fake = g_model(noise,training=False)

               
                
                pyplot.figure(figsize=(25,25))
                for i in range(n_samples):

                    pyplot.subplot(5, 5, 1 + i)
                    pyplot.axis('off')
                    pyplot.imshow(x_fake[i, :, :,0],cmap='gray')

                pyplot.show()
                pyplot.close()
            count=count+1

train(train_dataset, EPOCHS)

"""# **Display generated fake brain images**"""

noise = tf.random.normal([n_samples, latent_dim])
x_fake = g_model(noise,training=False)

pyplot.figure(figsize=(25,25))
for i in range(n_samples):
    pyplot.subplot(5, 5, 1 + i)
    pyplot.axis('off')
    pyplot.imshow(x_fake[i, :, :,0],cmap='gray')
    pyplot.savefig('0511 Epoch batch().png')
pyplot.show()
pyplot.close()
#filename = 'GAN_model_%03d.h5' 
#g_model.save(filename)

"""# **Model evaluation**

**Graph Generator and Discriminator Loss**
"""

plt.plot(d_loss_history, label='Discriminator loss')
plt.plot(g_loss_history, label='Generator loss')
plt.xlabel('EPOCHS')
plt.ylabel('Loss')
plt.legend()
plt.show()

"""**SSIM**"""

whichfake=3
ssim_noise=[]

for i in range(training_images.shape[0]):
    ssim_noise.append( ssim(training_images[i,:,:,0], x_fake.numpy()[whichfake,:,:,0],data_range=np.max(x_fake.numpy()[whichfake,:,:,0]) - np.min(x_fake.numpy()[whichfake,:,:,0])))

fig, axs = pyplot.subplots(2, 1, constrained_layout=True,figsize=(10,10))
axs[0].imshow(x_fake[whichfake, :, :, 0],cmap="gray")
axs[0].set_title('Generated image with max SSIM: {:.4f}%'.format(np.max(ssim_noise)*100))

axs[1].imshow(training_images[ssim_noise.index(np.max(ssim_noise)), :, :, 0],cmap="gray")
axs[1].set_title('Closest PET image {:.0f}'.format(ssim_noise.index(np.max(ssim_noise))))

pyplot.show()